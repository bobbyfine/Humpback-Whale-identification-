{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PyTorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "1n3SsmJf1HMX"
      },
      "cell_type": "markdown",
      "source": [
        "# Training a ConvNet PyTorch\n",
        "\n",
        "In this notebook, you'll learn how to use the powerful PyTorch framework to specify a conv net architecture and train it on the CIFAR-10 dataset."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wGt5-Qdh1HMZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import timeit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xxGP1n-6_0-4",
        "colab_type": "code",
        "outputId": "6b17f6d6-cd27-44d4-a97d-246403356023",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9822525e-8afa-4c61-8b1e-4728d2615ef8\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9822525e-8afa-4c61-8b1e-4728d2615ef8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{u'kaggle.json': '{\"username\":\"dainiaozhou\",\"key\":\"7000702f342133c038b6be791b599e60\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "DIIadGSQL0kU",
        "colab_type": "code",
        "outputId": "3efe8b33-6d90-48e3-9781-562f300ce33c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LYyU5mBfBnYH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xnAAjYhtButp",
        "colab_type": "code",
        "outputId": "dac3c447-1b04-4d33-98ab-9cdb654a5475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oCK5_jrFBzE2",
        "colab_type": "code",
        "outputId": "9f6a8fad-1cd3-42c5-ee63-9a2c9722c21e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -l ~/.kaggle\n",
        "!cat ~/.kaggle/kaggle.json"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "-rw------- 1 root root 67 Mar 20 21:17 kaggle.json\n",
            "{\"username\":\"dainiaozhou\",\"key\":\"7000702f342133c038b6be791b599e60\"}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6ugLmNC-B8JA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!pip install -q kaggle-cli"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UnmBqXl5CCY5",
        "colab_type": "code",
        "outputId": "e6b8c2e6-b744-4483-bbe0-2a6bbba484e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c humpback-whale-identification"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sample_submission.csv to /content\n",
            "\r  0% 0.00/498k [00:00<?, ?B/s]\n",
            "100% 498k/498k [00:00<00:00, 34.7MB/s]\n",
            "Downloading train.csv to /content\n",
            "  0% 0.00/594k [00:00<?, ?B/s]\n",
            "100% 594k/594k [00:00<00:00, 80.6MB/s]\n",
            "Downloading test.zip to /content\n",
            "100% 1.34G/1.35G [00:20<00:00, 78.4MB/s]\n",
            "100% 1.35G/1.35G [00:20<00:00, 71.9MB/s]\n",
            "Downloading train.zip to /content\n",
            "100% 4.15G/4.16G [01:06<00:00, 78.6MB/s]\n",
            "100% 4.16G/4.16G [01:06<00:00, 67.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mosXcjxWC-Tp",
        "colab_type": "code",
        "outputId": "b297a4af-3b3b-4afd-d4d8-19c0f361f311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json   sample_submission.csv  train.csv\n",
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/  test.zip               train.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ec_rz_o4d5dc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip train.zip -d train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uahAOUXrEq0n",
        "colab_type": "code",
        "outputId": "77e25868-c144-40ee-d02c-18af3ecdc5c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"./\"]).decode(\"utf8\"))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n",
            "sample_data\n",
            "sample_submission.csv\n",
            "test.zip\n",
            "train\n",
            "train.csv\n",
            "train.zip\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G7EiBHH6FH_L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from IPython.display import display\n",
        "from IPython.display import Image as _Imgdis\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from time import time\n",
        "from time import sleep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HruimjAOFcH6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sXVmTVmt1Gn-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "from PIL import Image\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hxQeh3Pg1Snv",
        "colab_type": "code",
        "outputId": "96773035-0ce6-4827-fde0-2fb9f903f657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(os.listdir(\".\"))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'sample_submission.csv', 'test.zip', 'train.csv', 'kaggle.json', 'train.zip', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D1Xr_EaQ1ZCe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_df = pd.read_csv('./train.csv')\n",
        "submission_df = pd.read_csv('./sample_submission.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1L3m3IZgBVjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_df = label_df[label_df['Id'] != \"new_whale\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Mt_PlAvORtJ",
        "colab_type": "code",
        "outputId": "c840f0ab-a77d-41b5-ea5c-65ad7b012dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "cell_type": "code",
      "source": [
        "count = dict()\n",
        "for index, row in label_df.iterrows():\n",
        "  if row['Id'] not in count.keys():\n",
        "    count[row['Id']] = 1\n",
        "  else:\n",
        "    count[row['Id']] += 1\n",
        "    \n",
        "\n",
        "good_ids = []\n",
        "for id, c in count.items():\n",
        "  if c >= 31:\n",
        "    good_ids.append(id)\n",
        "print(good_ids)\n",
        "print(len(good_ids))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['w_6cda039', 'w_f765256', 'w_af367c3', 'w_0369a5c', 'w_5e8e218', 'w_3de579a', 'w_9c506f6', 'w_9b5109b', 'w_f0fe284', 'w_2b069ba', 'w_1ca9ab1', 'w_d405854', 'w_6822dbc', 'w_d72771c', 'w_fd3e556', 'w_23a388d', 'w_a9304b9', 'w_778e474', 'w_700ebb4', 'w_60ce6fc', 'w_789c969', 'w_5a2634c', 'w_8c25681', 'w_564a34b', 'w_17b0d3a', 'w_343f088', 'w_08630fd', 'w_88e4537']\n",
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W8GxvgWs69rH",
        "colab_type": "code",
        "outputId": "dc3b80fd-cf56-48c6-cbf0-1bec753fc03f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "for index, row in label_df.iterrows():\n",
        "  if row['Id'] in good_ids:\n",
        "    dataset.append((row['Id'],row['Image']))\n",
        "print(dataset[1])\n",
        "print(len(dataset))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('w_9c506f6', '00570db6b.jpg')\n",
            "1235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B8mJ6139dc29",
        "colab_type": "code",
        "outputId": "baba5c27-f8f0-41ad-97fa-c1687439e27c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        }
      },
      "cell_type": "code",
      "source": [
        "label = dict()\n",
        "class_label = 0\n",
        "for whale_id,_ in dataset:\n",
        "  if not whale_id in label.keys():\n",
        "    label[whale_id] = class_label\n",
        "    class_label += 1\n",
        "print(label)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'w_789c969': 23, 'w_1ca9ab1': 8, 'w_6cda039': 19, 'w_08630fd': 7, 'w_778e474': 4, 'w_a9304b9': 6, 'w_d72771c': 10, 'w_6822dbc': 2, 'w_9b5109b': 15, 'w_5a2634c': 26, 'w_17b0d3a': 24, 'w_3de579a': 0, 'w_fd3e556': 18, 'w_d405854': 12, 'w_9c506f6': 1, 'w_88e4537': 20, 'w_f0fe284': 9, 'w_8c25681': 25, 'w_564a34b': 13, 'w_700ebb4': 3, 'w_343f088': 21, 'w_f765256': 16, 'w_0369a5c': 5, 'w_af367c3': 22, 'w_5e8e218': 14, 'w_60ce6fc': 17, 'w_2b069ba': 27, 'w_23a388d': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A70kiSa23BQL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_pad_width(im, new_shape, is_rgb=True):\n",
        "    pad_diff = new_shape - im.shape[0], new_shape - im.shape[1]\n",
        "    t, b = int(math.floor(pad_diff[0]/2)), int(math.ceil(pad_diff[0]/2))\n",
        "    l, r = int(math.floor(pad_diff[1]/2)), int(math.ceil(pad_diff[1]/2))\n",
        "    if is_rgb:\n",
        "        pad_width = ((t,b), (l,r), (0, 0))\n",
        "    else:\n",
        "        pad_width = ((t,b), (l,r))\n",
        "    return pad_width\n",
        "\n",
        "def pad_and_resize_cv(image_path, dataset, desired_size=224):\n",
        "    img = cv2.imread('./' + dataset + '/' + image_path )\n",
        "    \n",
        "    pad_width = get_pad_width(img, max(img.shape))\n",
        "    padded = np.pad(img, pad_width=pad_width, mode='constant', constant_values=0)\n",
        "    \n",
        "    resized = cv2.resize(padded, (desired_size,)*2).astype('uint8')\n",
        "    \n",
        "    return resized\n",
        "\n",
        "def pad_and_resize_pil(image_path, dataset, desired_size=224):\n",
        "    '''Experimental'''\n",
        "    im = Image.open('./' + dataset + '/' + image_path)\n",
        "    \n",
        "    old_size = im.size\n",
        "    ratio = float(desired_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "    resized = im.resize(new_size)\n",
        "    im_array = np.asarray(resized)\n",
        "    \n",
        "    pad_width = get_pad_width(im_array, desired_size)\n",
        "    padded = np.pad(im_array, pad_width=pad_width, mode='constant', constant_values=0)\n",
        "    \n",
        "    return padded\n",
        "\n",
        "\n",
        "def pad_and_resize(image_path, dataset, desired_size=128, mode='cv'):\n",
        "    if mode =='pil':\n",
        "        return pad_and_resize_pil(image_path, dataset, desired_size)\n",
        "    else:\n",
        "        return pad_and_resize_cv(image_path, dataset, desired_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uLB8HM_24ICF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_resized_imgs = []\n",
        "test_resized_imgs = []\n",
        "\n",
        "for id,image_path in dataset:\n",
        "    train_resized_imgs.append(pad_and_resize(image_path, 'train'))\n",
        "\n",
        "## for image_path in submission_df['Image']:\n",
        "    ## test_resized_imgs.append(pad_and_resize(image_path, 'test'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WGTE-dbpsKir",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(train_resized_imgs)):\n",
        "  train_resized_imgs[i] = np.transpose(train_resized_imgs[i], (2,1,0))\n",
        "  train_resized_imgs[i] = torch.from_numpy(train_resized_imgs[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e1ORp5juQx2H",
        "colab_type": "code",
        "outputId": "b13066b0-3e42-43ef-cd53-1ab6b919c65b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_resized_imgs[0].shape)\n",
        "print(len(train_resized_imgs))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 128, 128])\n",
            "1235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3_SuMOMhfTe3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vq4CO8--ejef",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_set = random.sample(xrange(1235),235)\n",
        "train_set = list(set(range(1235)) - set(val_set))\n",
        "\n",
        "# val_set = range(1000,1235)\n",
        "# train_set = range(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gWrpfXzAefuB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "partitions = {\"train\":train_set, \"validation\": val_set}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZpNvzlEcSx5R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, list_IDs, labels):\n",
        "        'Initialization'\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.list_IDs[index]\n",
        "\n",
        "        # Load data and get label\n",
        "        X = train_resized_imgs[ID]\n",
        "        y = self.labels[dataset[ID][0]]\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yRY11ADFUjEC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params = {'batch_size': 64,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 6}\n",
        "\n",
        "\n",
        "# Generators\n",
        "training_set = Dataset(partitions['train'], label)\n",
        "training_generator = data.DataLoader(training_set, **params)\n",
        "\n",
        "validation_set = Dataset(partitions['validation'], label)\n",
        "validation_generator = data.DataLoader(validation_set, **params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f3ZVFsOf1HMt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size() # read in N, C, H, W\n",
        "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "O-VQi0oe1HND",
        "outputId": "725309e3-b476-472d-e213-71803cce9069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Verify that CUDA is properly configured and you have a GPU available\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NVXRmzZ11HNg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gpu_dtype = torch.cuda.FloatTensor\n",
        "def train(model, loss_fn, optimizer, num_epochs = 1):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
        "        model.train()\n",
        "        for t, (x, y) in enumerate(training_generator):\n",
        "            x_var = Variable(x.type(gpu_dtype))\n",
        "            y_var = Variable(y.type(gpu_dtype).long())\n",
        "\n",
        "\n",
        "            scores = model(x_var)\n",
        "            \n",
        "            loss = loss_fn(scores, y_var)\n",
        "            if (t + 1) % print_every == 0:\n",
        "                print('t = %d, loss = %.4f' % (t + 1, loss.data.item()))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "def check_accuracy(model, loader):\n",
        "    #if loader.dataset.train:\n",
        "        #print('Checking accuracy on validation set')\n",
        "    #else:\n",
        "        #print('Checking accuracy on test set')   \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
        "    for x, y in loader:\n",
        "        x_var = Variable(x.type(gpu_dtype), volatile=True)\n",
        "\n",
        "        scores = model(x_var)\n",
        "        _, preds = scores.data.cpu().max(1)\n",
        "        num_correct += (preds == y).sum()\n",
        "        num_samples += preds.size(0)\n",
        "    acc = float(num_correct) / num_samples\n",
        "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KDVN6AUkzq_7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print_every = 10\n",
        "\n",
        "# This is a little utility that we'll use to reset the model\n",
        "# if we want to re-initialize all our parameters\n",
        "def reset(m):\n",
        "    if hasattr(m, 'reset_parameters'):\n",
        "        m.reset_parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "G-Oie4471HNs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train your model here, and make sure the output of this cell is the accuracy of your best model on the \n",
        "# train, val, and test sets. Here's some code to get you started. The output of this cell should be the training\n",
        "# and validation accuracy on your best model (measured by validation accuracy).\n",
        "gpu_dtype = torch.cuda.FloatTensor\n",
        "\n",
        "# model_base = nn.Sequential (\n",
        "#     nn.Conv2d(3,32, kernel_size = 7),\n",
        "#     nn.LeakyReLU(inplace = True),\n",
        "#     nn.Conv2d(32,32, kernel_size = 3),\n",
        "#     nn.LeakyReLU(inplace = True),\n",
        "#     nn.Conv2d(32,32, kernel_size = 3),\n",
        "#     nn.LeakyReLU(inplace = True),\n",
        "#     nn.BatchNorm2d(32),\n",
        "#     nn.MaxPool2d(kernel_size=4, stride = 4),\n",
        "#     nn.Dropout(0.30),\n",
        "    \n",
        "#     Flatten(),\n",
        "#     nn.Linear(26912,1280),\n",
        "#     nn.ReLU(inplace = True),\n",
        "#     nn.Dropout(),\n",
        "#     nn.Linear(1280,28),\n",
        "#       )\n",
        "\n",
        "model_base_1 = nn.Sequential (\n",
        "    nn.Conv2d(3,32, kernel_size = 3),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "    \n",
        "    nn.Conv2d(32,32, kernel_size = 3),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "    \n",
        "    nn.Conv2d(32,64, kernel_size = 3),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "    \n",
        "    nn.Conv2d(64,128, kernel_size = 3),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.MaxPool2d(kernel_size=4, stride = 4),\n",
        "    \n",
        "    \n",
        "    nn.Dropout(0.20),\n",
        "    \n",
        "    \n",
        "    \n",
        "    Flatten(),\n",
        "    nn.Linear(1152,128),\n",
        "    nn.ReLU(inplace = True),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128,28)\n",
        ")\n",
        "\n",
        "\n",
        "model_1 = model_base_1.type(gpu_dtype)\n",
        "loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
        "optimizer = optim.RMSprop(model_1.parameters(), lr = 1e-3)\n",
        "\n",
        "train(model_1, loss_fn, optimizer, num_epochs=50)\n",
        "check_accuracy(model_1, validation_generator)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bQP_7-X9qZye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "outputId": "10746eb5-db84-447e-8195-01b7f362967d"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "check_accuracy(model_1, validation_generator)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:30: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 141 / 235 correct (60.00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r8MJbPttUPfG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_base_2 = nn.Sequential (\n",
        "    nn.Conv2d(3,32, kernel_size = 3),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.MaxPool2d(kernel_size=4, stride = 4),\n",
        "    \n",
        "    nn.Conv2d(32,64, kernel_size = 3),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "    \n",
        "    nn.Conv2d(64,64, kernel_size = 3),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "    \n",
        "    nn.Conv2d(64,128, kernel_size = 3),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "    \n",
        "    \n",
        "    nn.Dropout(0.50),\n",
        "    \n",
        "    \n",
        "    \n",
        "    Flatten(),\n",
        "    nn.Linear(512,128),\n",
        "    nn.ReLU(inplace = True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(128,28)\n",
        ")\n",
        "\n",
        "\n",
        "model_2 = model_base_2.type(gpu_dtype)\n",
        "loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
        "optimizer = optim.RMSprop(model_2.parameters(), lr = 1e-3)\n",
        "\n",
        "train(model_2, loss_fn, optimizer, num_epochs=50)\n",
        "check_accuracy(model_2, validation_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rdP7r_OpFbeF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "outputId": "a5c78692-24a2-4929-b1d9-7d012fac4201"
      },
      "cell_type": "code",
      "source": [
        "check_accuracy(model_2, validation_generator)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got 145 / 235 correct (61.70)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:30: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "T_xJpbRabsGG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_base_3 = nn.Sequential (\n",
        "    nn.Conv2d(3,32, kernel_size = 7),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "    \n",
        "    \n",
        "    nn.Conv2d(32,64, kernel_size = 5),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.MaxPool2d(kernel_size=3, stride = 3),\n",
        "    \n",
        "    nn.Conv2d(64,128, kernel_size = 5),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "    \n",
        "    \n",
        "    nn.Dropout(0.40),\n",
        "    \n",
        "    \n",
        "    \n",
        "    Flatten(),\n",
        "    nn.Linear(6272,1024),\n",
        "    nn.ReLU(inplace = True),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(1024,128),\n",
        "    nn.ReLU(inplace = True),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(128,28)\n",
        ")\n",
        "\n",
        "\n",
        "model_3 = model_base_3.type(gpu_dtype)\n",
        "loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
        "optimizer = optim.RMSprop(model_3.parameters(), lr = 1e-3)\n",
        "\n",
        "train(model_3, loss_fn, optimizer, num_epochs=50)\n",
        "check_accuracy(model_3, validation_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cLBZyUxBGU8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "outputId": "cdb3e08d-af56-4e78-cef4-a7c7ca7409f6"
      },
      "cell_type": "code",
      "source": [
        "check_accuracy(model_3, validation_generator)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:30: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 106 / 235 correct (45.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "31OdmX4mdZeg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_base_4 = nn.Sequential (\n",
        "    nn.Conv2d(3,16, kernel_size = 5),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(16),\n",
        "    nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "    \n",
        "    nn.Conv2d(16,32, kernel_size = 5),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.Conv2d(32,64, kernel_size = 5),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.MaxPool2d(kernel_size=3, stride = 3),\n",
        "    \n",
        "    nn.Conv2d(64,128, kernel_size = 5),\n",
        "    nn.LeakyReLU(inplace = True),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.MaxPool2d(kernel_size=2, stride = 2),\n",
        "    \n",
        "    nn.Dropout(0.30),\n",
        "    \n",
        "    \n",
        "    Flatten(),\n",
        "    nn.Linear(6272,1024),\n",
        "    nn.ReLU(inplace = True),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(1024,128),\n",
        "    nn.ReLU(inplace = True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(128,28)\n",
        ")\n",
        "\n",
        "\n",
        "model_4 = model_base_4.type(gpu_dtype)\n",
        "loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
        "optimizer = optim.RMSprop(model_4.parameters(), lr = 1e-3)\n",
        "\n",
        "train(model_4, loss_fn, optimizer, num_epochs=50)\n",
        "check_accuracy(model_4, validation_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pOv_6_DaGyKx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "outputId": "7853b515-eae0-429e-82bd-76eb969eddd1"
      },
      "cell_type": "code",
      "source": [
        "check_accuracy(model_4, validation_generator)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:30: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 138 / 235 correct (58.72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "frrXkq6Ar3PK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "bf82b5f3-572d-4261-86b2-eb8a5284646d"
      },
      "cell_type": "code",
      "source": [
        "num_correct = 0\n",
        "num_samples = 0\n",
        "model_1.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
        "model_2.eval()\n",
        "model_3.eval()\n",
        "model_4.eval()\n",
        "for x, y in validation_generator:\n",
        "    x_var = Variable(x.type(gpu_dtype), volatile=True)\n",
        "\n",
        "    scores_1 = model_1(x_var)\n",
        "    scores_2 = model_2(x_var)\n",
        "    scores_3 = model_3(x_var)\n",
        "    scores_4 = model_4(x_var)\n",
        "    \n",
        "    _, preds_1 = scores_1.data.cpu().max(1)\n",
        "    _, preds_2 = scores_2.data.cpu().max(1)\n",
        "    _, preds_3 = scores_3.data.cpu().max(1)\n",
        "    _, preds_4 = scores_4.data.cpu().max(1)\n",
        "    \n",
        "    \n",
        "    for i in range(list(preds_1.shape)[0]):\n",
        "      preds[i] = np.argmax(np.bincount([preds_1[i],preds_2[i],preds_3[i],preds_4[i]]))\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    num_correct += (preds == y).sum()\n",
        "    num_samples += preds.size(0)\n",
        "acc = float(num_correct) / num_samples\n",
        "print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
        "\n",
        "print(preds_1[0])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got 152 / 235 correct (64.68)\n",
            "tensor(18)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}